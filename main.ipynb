{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from hunspell import Hunspell\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('metadata/final_1.csv')\n",
    "filenames = data['filename'].to_list()\n",
    "years = data['years'].to_list()\n",
    "titles = data['title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "dataset = []\n",
    "\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "for filename in sorted_alphanumeric(os.listdir('txt')):\n",
    "    dataset.append(('txt/'+filename, titles[int(filenames.index(filename))]))\n",
    "\n",
    "N = len (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "def print_doc(id):\n",
    "    print(dataset[id])\n",
    "    file = open(dataset[id][0], 'r', encoding='utf8')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= SnowballStemmer(language='english')\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming2(data):\n",
    "    hobj = Hunspell('en_US')\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        if len(hobj.stem(w)) > 0:\n",
    "            new_text = new_text + \" \" + hobj.stem(w)[0]\n",
    "        else:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming2(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming2(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "processed_text = []\n",
    "processed_title = []\n",
    "\n",
    "for i in dataset[:N]:\n",
    "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "with open('extracted_data/processed_text.data', 'wb') as f:\n",
    "    pickle.dump(processed_text, f)\n",
    "with open('extracted_data/processed_title.data', 'wb') as f:\n",
    "    pickle.dump(processed_title, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_data/processed_text.data', 'rb') as f:\n",
    "    processed_text = pickle.load(f)\n",
    "with open('extracted_data/processed_title.data', 'rb') as f:\n",
    "    processed_title = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "DF = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "total_vocab_size = len(DF)\n",
    "total_vocab = [x for x in DF]\n",
    "def get_doc_freq(word):\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        c = 0\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1447\n",
      "2908\n"
     ]
    }
   ],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "doc = 0\n",
    "\n",
    "TF_IDF = {}\n",
    "\n",
    "files_list = []\n",
    "total_count_all = 0\n",
    "idf_all = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text[i]\n",
    "    counter = Counter(tokens + processed_title[i]) # Because we need to sum up all the word from title and body, anw it's about unique word and its counter of each word\n",
    "    words_count = len(tokens + processed_title[i]) # Same thing, btw it's about all the words appeared, even duplicated\n",
    "    tfs = {}\n",
    "    counters = {}\n",
    "    total_count = 0\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token]/words_count\n",
    "        tfs.update({token:tf})\n",
    "        counters.update({token:counter[token]})\n",
    "        total_count += counter[token]\n",
    "        df = get_doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1)) # Numerator is added 1 to avoid negative values\n",
    "        idf_all.update({token:idf}) \n",
    "        TF_IDF[doc, token] = tf*idf\n",
    "\n",
    "    tmp_dict = {'name': filenames[i], \"words_counting\":counters, \"total_count\":total_count, \"TF_scores\":tfs}\n",
    "    files_list.append(tmp_dict)\n",
    "\n",
    "    total_count_all += total_count\n",
    "    doc += 1\n",
    "    \n",
    "all_list = {'name': 'ALL_Documents', 'total_count': total_count_all, 'DF': DF, 'IDF_scores': idf_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "doc = 0\n",
    "\n",
    "TF_IDF_title = {}\n",
    "\n",
    "files_list_title = []\n",
    "total_count_all_title = 0\n",
    "idf_all_title = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    counter = Counter(tokens + processed_text[i])\n",
    "    words_count = len(tokens + processed_text[i])\n",
    "    tfs = {}\n",
    "    counters = {}\n",
    "    total_count = 0\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token]/words_count\n",
    "        tfs.update({token:tf})\n",
    "        counters.update({token:counter[token]})\n",
    "        total_count += counter[token]\n",
    "        df = get_doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1)) \n",
    "        TF_IDF_title[doc, token] = tf*idf\n",
    "\n",
    "    tmp_dict = {'name': filenames[i], \"words_counting\":counters, \"total_count\":total_count, \"TF_scores\":tfs}\n",
    "    files_list_title.append(tmp_dict)\n",
    "    doc += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "with open('extracted_data/all_docs_info.json', 'w') as f:\n",
    "    f.write(json.dumps(all_list, indent=4))\n",
    "with open('extracted_data/all_text_info.json', 'w') as f:\n",
    "    f.write(json.dumps(files_list, indent=4))\n",
    "with open('extracted_data/all_title_info.json', 'w') as f:\n",
    "    f.write(json.dumps(files_list_title, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "tf_idf_tmp = copy.deepcopy(TF_IDF)\n",
    "tf_idf_title_tmp = copy.deepcopy(TF_IDF_title)\n",
    "# tf_idf_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "TF_IDF = copy.deepcopy(tf_idf_tmp)\n",
    "TF_IDF_title = copy.deepcopy(tf_idf_title_tmp)\n",
    "alpha = 0.4\n",
    "for i in TF_IDF:\n",
    "    TF_IDF[i] *= alpha\n",
    "for i in TF_IDF_title:\n",
    "    try:\n",
    "        TF_IDF[i] += TF_IDF_title[i] * (1-alpha)\n",
    "    except KeyError:\n",
    "        continue\n",
    "TF_IDF_tmp = copy.deepcopy(TF_IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "vectorized_array = np.zeros((N, total_vocab_size))\n",
    "for i in TF_IDF_tmp:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        vectorized_array[i[0]][ind] = TF_IDF_tmp[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_data/all_docs_info.json', 'r') as f:\n",
    "    DF_tmp = json.load(f).get('DF')\n",
    "    \n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "def get_doc_freg_for_vector(token):\n",
    "    try:\n",
    "        c = DF_tmp[token]\n",
    "    except:\n",
    "        c = 0\n",
    "    return c\n",
    "\n",
    "def gen_vector(tokens):\n",
    "    total_vocab = [x for x in DF_tmp]\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = get_doc_freg_for_vector(token)\n",
    "        idf = math.log((len(processed_text)+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "doc_id = 0\n",
    "score_dict = {}\n",
    "tmp_dict_tfidf = {}\n",
    "tfidf_list = []\n",
    "for i in TF_IDF:\n",
    "    if i[0] != doc_id:\n",
    "        tmp_dict_tfidf.update({\"name\": filenames[doc_id], \"scores\": score_dict})\n",
    "        tfidf_list.append(tmp_dict_tfidf)\n",
    "        doc_id = i[0]\n",
    "        tmp_dict_tfidf = {}\n",
    "        score_dict = {}\n",
    "    doc_name = filenames[i[0]]\n",
    "    token = i[1]\n",
    "    score_dict.update({token:TF_IDF[i]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only runs when need to reload the dataset\n",
    "with open('extracted_data/tf_idf_scores.json', 'w') as f:\n",
    "    f.write(json.dumps(tfidf_list, indent=4))\n",
    "with open('extracted_data/tf_idf_vectorized.npy', 'wb') as f:\n",
    "    np.save(f, vectorized_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_data/tf_idf_scores.json\", 'r', encoding=\"utf8\") as j:\n",
    "    TF_IDF = json.loads(j.read())\n",
    "vectorized_array = np.load('extracted_data/tf_idf_vectorized.npy')\n",
    "# TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: Ho Chi Minh the Greatest General\n",
      "\n",
      "['ho', 'chi', 'minh', 'great', 'general']\n",
      "\n",
      "['42.txt', '8.txt', '73.txt', '87.txt', '26.txt', '27.txt', '105.txt', '44.txt', '90.txt', '35.txt']\n"
     ]
    }
   ],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for idx, file in enumerate(TF_IDF):\n",
    "        scores = file.get('scores')\n",
    "        for key in scores:\n",
    "            if key in tokens:\n",
    "                try:\n",
    "                    query_weights[idx] += scores[key]\n",
    "                except:\n",
    "                    query_weights[idx] = scores[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:k]:\n",
    "        l.append(filenames[i[0]])\n",
    "    \n",
    "    print(l)\n",
    "    \n",
    "\n",
    "matching_score(10, \"Ho Chi Minh the Greatest General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "\n",
      "Query: Ho Chi Minh the Greatest General\n",
      "\n",
      "['ho', 'chi', 'minh', 'great', 'general']\n",
      "\n",
      "42.txt 8.txt 35.txt 87.txt 26.txt 90.txt 73.txt 27.txt 105.txt 81.txt "
     ]
    }
   ],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in vectorized_array:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for idx in out:\n",
    "        print(filenames[idx], end=\" \")\n",
    "\n",
    "#     for i in out:\n",
    "#         print(i, dataset[i][0])\n",
    "\n",
    "Q = cosine_similarity(10, \"Ho Chi Minh the Greatest General\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c8a597d643227bf884d1e4c3a9f829081b1ebbbece9c97331df2a70b5eb27e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('csdldpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
